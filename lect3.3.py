# Переобучение и дисперсия

# Линейная регрессия - сумма квадратов
# Цель состоит не в минимизации суммы квадратов, а в том, чтобы делать
# "правильные" предсказания на новых данных.

# Переобучение модели - чувствительны к выбросам, которые находятся далеко от
# остальных точек, в прогнозах будет высокая дисперсия.
# Поэтому к моделям специально добавляется смещение.

# Смещение модели означает, что предпочтение отдается определенной схеме
# (например, прямая линия), а не графикам со сложной структурой. минимизирующей сумму остатков.
# Если вносим смещение, можем недоучить модель.

# Задача - балансировака между минимизацией функции потерь (ведет к
# переобучению), смеЩение (ведет к недообучению).

# - гребневая регрессия (Ridge) добавляет смещение в виде штрафа, из-за этого хуже идет подгонка.
# - Лассо - регрессия - удаление некоторых переменных.

# Механически применить линейную регрессию к данным, сделать на основе
# полученной модели прогноз, считать, что все в порядке, нельзя.
# Даже если регрессию свести к прямой линии, она может быть переобученной.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split, KFold, cross_val_score


data = np.array([
    [1, 5],
    [2, 7],
    [3, 7],
    [4, 10],
    [5, 11],
    [6, 14],
    [7, 17],
    [8, 19],
    [9, 22],
    [10, 28],
])
# Градиентный спуск - пакетный градиентный спуск. => для работы используются все обучающие данные.
# На практике - стохастический градиентный спуск - на каждой итерации обучаемся только на одной выборке из данных
# - сокращение числа вычислений
# - вносим смещение, боремся с переобучением
# Мини-пакетный градиентный спуск: на каждоый итерации используется несколько выборок
x = data[:, 0]
y = data[:, 1]
n = len(x)

w1 = 0.0
w0 = 0.0

L = 0.001
# размер выборки - какой?
sample_size = 2
# на каждой итерации используем один элемент
iterations = 100_000
for i in range(iterations):
    idx = np.random.choice(n, sample_size, replace=False)
    D_w0 = 2 * sum(-y[idx] + w0 + w1 * x[idx])
    D_w1 = 2 * sum(x[idx] * (-y[idx] + w0 + w1 * x[idx]))
    w1 -= L * D_w1
    w0 -= L * D_w0

print(w1, w0)

# Как оценить, насколько сильно прогнозы "промахиваются" прогнозы при использовании линейной регрессии?
# Для оценки взаимосвязи использовали линейный коэффициент корреляции.

data_df = pd.DataFrame(data)
print(data_df.corr(method='pearson'))

data_df[1] = data_df[1].values[::-1]

print(data_df.corr(method='pearson'))

# Коэффициент корреляции дает понять, есть ли связь между двумя переменными.

# Обучающая и тестовая выборки
# Основной метод борьбы с переобучением, заключается в том, что набор данных делится на обучающую и тестовую выборки.
# Во всех видах машинного обучения с учителем это встречается.
# Обычная пропорция - 2/3  на обучение, 1/3 на тест. (4/5, 9/10)

X = data_df.values[:, :-1]
Y = data_df.values[:, -1]

# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3)

kfold = KFold(n_splits=3, random_state=1, shuffle=True)

model = LinearRegression()
# model.fit(X_train, Y_train)

results = cross_val_score(model, X, Y, cv=kfold)

print(results)
print(results.mean(), results.std())
# Коэффициент детерминации
# 0 < r^2 < 1
# Чем ближе к 1, тем лучше регрессия работает на тестовых данных.
# Перекрестная валидация...
# r = model.score(X_test, Y_test)
# print(r)

# Метрики показывают, насколько ЕДИНООБРАЗНО ведет себя модель на разных выборках
# Возможно использование поэлементной перекрестной валидации
# Можно делать случайную валидацию

# Валидационная выборка - несколько модельных конфигураций 

# Многомерная линейная регрессия

data_df = pd.read_csv('./multiple_independent_variable_linear.csv')
print(data_df.head())

X = data_df.values[:, :-1]
Y = data_df.values[:, -1]

model = LinearRegression().fit(X, Y)

print(model.coef_, model.intercept_)

x1 = X[:, 0]
x2 = X[:, 1]
y = Y

fig = plt.figure()
ax = plt.axes(projection='3d')
ax.scatter3D(x1, x2, y)

x1_ = np.linspace(min(x1), max(x1), 100)
x2_ = np.linspace(min(x2), max(x2), 100)

X1_, X2_ = np.meshgrid(x1_, x2_)

Y_ = model.intercept_ + model.coef_[0] * X1_ + model.coef_[1] * X2_

ax.plot_surface(X1_, X2_, Y_, cmap='Greys', alpha=0.6)

plt.show()