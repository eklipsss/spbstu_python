# Машинное обучение

# ML - решает следующую задачу
# Требуется подогнать заданный набор точек данными под соответствующую функцию
# (отображение входа на выход), которая улавливает важные сигналы в данных и
# игнорирует помехи, а затем убедиться, что на новых данных функция работает
# хорошо.

# Источник данных -> Данные -> Обучение -> Модель
# Модель можно проверить: на основе данных получить результат. 

# Обучение с учителем (supervised learning)
# Обучение без учителя (unsupervised learning)

# ОчУ - моделирует отношение между признаками и метками. Такие модели служат для предсказания меток на основе маркированных обучающих данных. После построения модели можно использовать ее для присвоения меток новым, ранее неизвестным данным.

# - задачи классификации (метки - дискретные: два или более)
# - задачи регрессии (метки/результат, непрерывные величины) 

# ОбУ - моделирование признаков без меток. Такие модели служат для выявления структуры немаркированных данных.

# - задача кластеризации (выделяет отдельные группы данных)
# - понижения размерности (поиск более сжатого представления данных)

# Существуют методы частичного обучения (semi-supervised learning). Не все данные промаркированы

# Методы обучения с подкреплением (reinforcement learning). Система обучения улучшает свои характеристики на основе взаимодействия (обратной связи) со средой. При этом взаимодействии система получает сигналы (функции наград), которые несут в себе информацию насколько хорошо/плохо система решила задачу (с точки зрения среды). Итоговая награда не станет максимальной.

import seaborn as sns

iris = sns.load_dataset('iris')

print(iris.head())
print(type(iris))

print(type(iris.values))

print(iris.values.shape)
print(iris.columns)
print(iris.index)

# Строки - отдельные объекты - образцы (sample)
# Столбцы - признаки (features) - соответствуют конкретным наблюдениям
# Матрицы признаков (features matrix) размер - число образцов * число признаков
# Целевой массив, массив меток (targets) - одномерный массив (1 * число образцов) - данные, которые мы хотим предсказать на основе имеющихся данных
# Зависимые (метка) и независимые переменные (признаки)


# Процесс построения системы машинного обучения

# 1. Предварительная обработка
# - на вход поступают необработанные данные и метки
# - происходит выбор признаков, масштабирование признаков
# - понижение размерности
# - выборка образцов
# - на выход набор данных: обучающий, тестовый

# 2. Обучение
# - выбор модели
# - перекрестная проверка
# - метрики эффективности
# - оптимизация гиперпараметров. Параметры, которые получаются не из данных. а являются характеристиками модели

# 3. Оценка и формирование финальной модели

# 4. Прогнозирование (использование модели)

# Scikit-learn

# 1. Выбираем класс модели
# 2. Устанавливаем гиперпараметры модели
# 3. Создаем матрицу признаков и целевой массив
# 4. Обучение модели fit()
# 5. Применить модель к новым данным
# - predict() (с учителем)
# - predict() или transform() (без учителя)


# Обучение с учителем: линейная регрессия

# Простая линейная регрессия

# y = ax + b

import matplotlib.pyplot as plt
import numpy as np


np.random.seed(1)
x = 10 * np.random.rand(50)
y = 2 * x - 1 + np.random.randn(50)

plt.scatter(x, y)

# II

# 1. Выбираем класс модели
from sklearn.linear_model import LinearRegression

# 2. Гиперпараметры модели
model = LinearRegression(fit_intercept=False)

# 3. Создаем матрицу признаков и целевой массив

print(x.shape)
print(y.shape)

X = x[:, np.newaxis]

# 4. Обучение модели fit()
model.fit(X, y)

print(model.coef_[0])
print(model.intercept_)

x_ = np.linspace(0, 10, 30)
y_ = model.coef_[0] * x_ + model.intercept_
plt.plot(x_, y_)

# 5. Применить модель к новым данным

xfit = np.linspace(-10, 10, 5)
yfit = model.predict(xfit[:, np.newaxis])

plt.scatter(xfit, yfit)

plt.show()